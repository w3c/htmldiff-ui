#!/usr/bin/python3
# -*- coding: utf-8 -*-
# MIT License
# © W3C® (MIT, ERCIM, Keio, Beihang).
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice (including the next paragraph) shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

import atexit
import html
import os
import re
import sys
import shutil
import tempfile
import urllib.parse
import urllib.error
import gzip
from subprocess import Popen, PIPE
import http.client
from bs4 import BeautifulSoup

import http_auth
import html5lib

CONTENT_TYPE = "text/html;charset=utf-8"

Page = """
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US">
<head><title>HTML Diff service</title>
<link rel="stylesheet" href="http://www.w3.org/StyleSheets/base" />
</head>
<body>

<p><a href="http://www.w3.org/"><img src="http://www.w3.org/Icons/w3c_home" alt="W3C"/></a> <a href="http://www.w3.org/2003/Editors">W3C Editors homepage</a></p>

<h1>Create Diff between HTML pages</h1>
"""
Page2 = """
<form method="GET">
<p>Address of reference document: <input name="doc1" type="url" value="%s" style="width:100%%"/></p>
<p>Address of new document: <input name="doc2" value="%s"  style="width:100%%"/></p>
<p><input type="submit" value="get Diff"/></p>
</form>

<p><strong>Tip</strong>: if the document uses the W3C convention on linking to its previous version, you can specify only the address of the new document — the previous link will be automatically detected.</p>
<h2>Diff markings</h2>
<p>This service relies on <a href="https://www.gnu.org/software/diffutils/">GNU diff</a>. The found differences are roughly marked as follow:
<ul>
<li>deleted text is shown in pink with down-arrows (as styled for a &lt;del> element)</li>
<li>where there is replacement, it’s shown in green with bi-directional arrows,</li>
<li>where there is newly inserted text, it’s yellow with up arrows (&lt;ins> element)</li>
</ul>
<address>
script $Revision$ of $Date$<br />
by <a href="http://www.w3.org/People/Dom/">Dominique Hazaël-Massieux</a><br />based on <a href="https://github.com/w3c/htmldiff-ui/blob/master/htmldiff.pl">Shane McCarron’ Perl script</a> wrapped in a <a href="https://github.com/w3c/htmldiff-ui/blob/master/htmldiff">Python CGI</a>
</address>
</body>
</html>
"""

class InvalidContentType(Exception):
    "Raised when an HTTP GET returns a non text/html resource"

def validate_url(url):
    "returns true if a url is valid, false otherwise"
    if not url:
        return False

    parsed_url = urllib.parse.urlparse(url)

    # validate schema
    if parsed_url.scheme not in ['http', 'https']:
        return False

    # check domain name has at least two components)
    netloc_parts = parsed_url.netloc.split('.')
    if ( len(netloc_parts) < 2
         or len(netloc_parts[0]) == 0
         or len(netloc_parts[1]) == 0 ):
        return False

    return True

def checkInputUrl(url):

    if  url[:5] == 'file:' or len(urllib.parse.urlparse(url)[0])<2:
        print("Status: 403")
        print("Content-Type: text/plain")
        print()
        print("sorry, I decline to handle file: addresses")
        sys.exit()
    elif not validate_url(url):
        print("Status: 403")
        print("Content-Type: text/plain")
        print()
        url = html.escape(url, True)
        print(f"sorry, I decline to handle this address: {url}")
        sys.exit()

def copyHeader(copy_func, source, key, header_name=None):
    rv = False
    value = source.get(key)

    if value:
        if header_name is None:
            header_name = key
        copy_func(header_name, value)
        rv = True
    return rv

def setupRequest(config_parsed=None, surblchecker=None):
    opener = http_auth.ProtectedURLopener(config_parsed=config_parsed,
                                          surblchecker=surblchecker)
    copyHeader(opener.add_header, os.environ,
               'HTTP_IF_MODIFIED_SINCE', 'If-Modified-Since')
    return opener

def processResponse(response):
    """
    Extracts the headers and body returned by a succesful
    DirectorOpener HTTP GET request.

    Stores the body into a temporary file and returns the file and headers.
    Caller must delete the temporary file.
    If there's no response object, returns a (None, {}) tuple.
    """
    if response:

        if not checkResponseIsHTML(response):
            raise InvalidContentType("Content-Type is not text/html")

        tfp = tempfile.NamedTemporaryFile(prefix='htmldiff-', delete=False)
        filename = tfp.name
        # Copy the binary content of the response to the file
        shutil.copyfileobj(response, tfp)
        tfp.close()

        return (filename, response.headers)

    else:
        return (None, {})

def checkResponseIsHTML(response):
    """
    Returns True if headers have a Content-Type header
    and if this header is of type text/html; returns
    False otherwise.
    """
    rv = False

    if 'Content-Type' in response.headers:
        content_type = response.headers.get_content_type()
        if content_type and content_type == 'text/html':
            rv = True

    return rv

def tidyFile(file):
    file.seek(0)
    doc = html5lib.parse(file.read())
    file.close()
    file = tempfile.NamedTemporaryFile(
        mode='w+', prefix='htmldiff-', suffix='.html')
    atexit.register(file.close)
    walker = html5lib.getTreeWalker("etree")
    stream = walker(doc)
    s = html5lib.serializer.HTMLSerializer()
    prettyhtml = s.serialize(stream)
    for item in prettyhtml:
        file.write(item)
    file.flush()
    file.seek(0)
    return (file, [])

def matchPredecessorRel(rel):
    return rel and "predecessor-version" in rel.lower().split(" ")

def rmfile(filename):
    try:
        os.unlink(filename)
    except Exception:
        pass

def escapeURLQueryString(url=None):
    if url is None:
        return None

    unsplit = False

    url_split = urllib.parse.urlsplit(url)

    if url_split.query:
        fields = dict(urllib.parse.parse_qsl(url_split.query))
        escaped_qs = urllib.parse.urlencode(fields)
        url_split = url_split._replace(query=escaped_qs)
        unsplit = True

    if url_split.path and not url_split.path.isascii():
        escaped_path = urllib.parse.quote(url_split.path)
        url_split = url_split._replace(path=escaped_path)
        unsplit = True

    if unsplit:
        rv = urllib.parse.urlunsplit(url_split)
    else:
        rv = url

    return rv

def mirrorURL(url, opener):
    try:
        url = escapeURLQueryString(url)
        response = opener.open(url)
        filename, headers = processResponse(response)
    except urllib.error.HTTPError as error:
        opener.error = f"HTTP error: {error.code} {error.reason}"
    except urllib.error.URLError as error:
        opener.error = f"URL error: {str(error.reason)}"
    except OSError as error:
        opener.error = f"I/O error: {error.errno} {error.strerror}"
    except http.client.InvalidURL:
        opener.error = "Invalid URL submitted"
    except InvalidContentType:
        opener.error = "Content-Type is not text/html"
    except AttributeError:  # ProxyProtectedURLopener returned None.
        pass                # There's already an error set.
    else:
        atexit.register(rmfile, filename)

        if 'Content-Type' in headers:
            charset = headers.get_content_charset()
            if charset:
                charset = charset.lower()
        else:
            charset = None

        if "content-encoding" in headers:
            if headers["content-encoding"] == "gzip":
                file = gzip.open(filename, encoding=charset)
            else:
                opener.error = f"Unsupported Content Encoding: {headers['content-encoding']}"
                # would be better to raise an exception...
                return (None, {})
        else:
            file = open(filename, encoding=charset)

        file, errors = tidyFile(file)
        if len(errors) == 0:
            return (file, headers)
        else:
            opener.error = "Tidy errors: %s" % (str(errors))
    return (None, {})

def showPage(url1='', url2='', error_html='', **headers):
    for name, value in list(headers.items()):
        print("{}: {}".format(name.replace('_', '-'), value))
    print()
    print(Page)
    print(error_html)
    print(Page2 % (url1, url2))
    sys.exit()

def formatErrorMessage(url='', error=''):
    if error:
        error = html.escape(error)

    if url:
        error_msg = f"<p style='color:#FF0000'>An error ({error}) occured trying to get <a href='{url}'>{url}</a>.</p>"
    else:
        error_msg = f"<p style='color:#FF0000'>An error ({error}) occured.</p>"

    return error_msg

def serveRequest():

    environ = os.environ
    if 'QUERY_STRING' not in environ:
        showPage(Content_Type=CONTENT_TYPE)

    # Use dict(parse_qsl) so we don't get lists of values.
    fields = dict(urllib.parse.parse_qsl(environ['QUERY_STRING'],
                                         max_num_fields = 2))
    if 'doc2' not in fields:
        showPage(Content_Type=CONTENT_TYPE)

    # doc1 is not mandatory; we always load doc2 first. If doc1 is not specified,
    # we check if doc2 has a previous version link and use that for doc1
    doc2 = fields['doc2']
    checkInputUrl(doc2)

    url_opener2 = setupRequest()
    newdoc, newheaders = mirrorURL(doc2, url_opener2)
    esc2 = html.escape(doc2, True)

    if not newdoc:
        newerror = url_opener2.error
        # see note below in error for refdoc
        #if re.match("^[1234][0-9][0-9] ", http_error):
        #    print(f"Status: {http_error}")
        print("Status: 403")
        error = formatErrorMessage(esc2, newerror)
        showPage(url2=esc2, error_html=error,
                 Content_Type=CONTENT_TYPE)

    if 'doc1' in fields:
        doc1 = fields['doc1']
    else:
        soup = BeautifulSoup(newdoc.read(), "html.parser")
        newdoc.seek(0)
        try:
            doc1 = soup.find(string=re.compile("Previous Version",re.IGNORECASE)).findNext(name="a", attrs={"href":True})["href"]
        except:
            try:
                doc1 = soup.find(name=["a", "link"], attrs={"href":True, rel:matchPredecessorRel})["href"]
            except:
                doc1 = None

    if not doc1:
        print("Status: 403")
        error = formatErrorMessage(error="No reference document found")
        showPage(url2=esc2, error_html=error, Content_Type=CONTENT_TYPE)

    checkInputUrl(doc1)

    urlcomponents1 = urllib.parse.urlparse(doc1)
    urlcomponents2 = urllib.parse.urlparse(doc2)
    # if same domain, we can use the same urlopener
    # otherwise, we create a separate one
    if urlcomponents2[1] == urlcomponents1[1]:
        url_opener = url_opener2
    else:
        # reuse surblchecker instance and parsed config data
        # from url_opener2
        config_parsed = url_opener2.config_parsed
        surblchecker = url_opener2.surblchecker
        url_opener = setupRequest(config_parsed=config_parsed,
                                  surblchecker=surblchecker)

    refdoc, refheaders = mirrorURL(doc1, url_opener)
    esc1 = html.escape(doc1, True)

    if not refdoc:
        error = url_opener.error

        # we used to catch the HTTP code and return it to the user
        # but this can lead to infinite authentication loops as
        # this script doesn't handle atm authentication.
        # Instead, if a request resulted in an HTTP error,
        # we're going to systematically return 403 so we can
        # have more useful logs
        #if re.match("^[1234][0-9][0-9] ", http_error):
        #    print(f"Status: {http_error}")
        print("Status: 403")
        error = formatErrorMessage(esc1, error)
        showPage(esc1, esc2, error, Content_Type=CONTENT_TYPE)

    print("Content-Type: text/html")
    # JK: commented this block as it doesn't seem to do anything;
    # delete it after a while
    # if 'Content-Type' in newheaders:
    #     charset = newheaders.get_content_charset()
    #     if charset:
    #         charset = charset.lower()
    #         #if charset == "iso-8859-1":
    #         #    options["char_encoding"]='latin1'

    for proxy_header in ('Last-Modified', 'Expires'):
        if copyHeader(lambda header, value: sys.stdout.write(f"{header}: {value}"), newheaders, proxy_header):
            print()
    print()
    p = Popen(["/usr/local/bin/htmldiff", refdoc.name, newdoc.name],
              stdin=PIPE, stdout=PIPE, stderr=PIPE)
    sys.stdout.flush()
    sys.stderr.flush()
    (out, err) = p.communicate()
    p.stdin.close()

    res = out.decode("utf-8")
    base = urlcomponents2.scheme + "://" + urlcomponents2.netloc
    if re.search(r'<base', res, flags=re.IGNORECASE) == None:
        if re.search(r'<head', res, flags=re.IGNORECASE):
            res = re.sub(r'<head([^>]*)>', r'<head\1><base href="' + base + '">', res, flags=re.IGNORECASE)
        else:
            res = re.sub(r'<html([^>]*)>', r'<html\1><base href="' + base + '">', res, flags=re.IGNORECASE)

    if err:
        print("Status: 403")
        error = formatErrorMessage(error=f"An error occured when running <code>htmldiff</code> on the documents:</p><pre>{html.escape(err)}</pre>")
        showPage(esc1, esc2, error, Content_Type=CONTENT_TYPE)
    else:
        # html5lib strips the doctype, so we add it manually
        # cf https://github.com/w3c/htmldiff-ui/issues/4#issuecomment-642831382
        print("<!doctype html>")
        print(res)

if __name__ == '__main__':
    if 'SCRIPT_NAME' in os.environ:
        serveRequest()
